# Image classification and generation from start to finish
---
In this repo I will go through how you can use convolutional neural networks (CNN) for computer vision. I will go step by step, starting from the collection of images which is done through scraping, after which I discuss image classification by training a CNN, and lastly I consider image generation with a VAE. Because doing computer vision with CNN is very quickly becomes very difficult and computationally heavy when you want to consider more complicated images than those of the MNIST database, I will make use of transfer learning, which is where you take machine learning models that have already been trained by someone else to serve your own machine learning plans. Below is a description of all the contents of the folders in this repo. To make sure that you have all the necessary python packages, use pip install -r requirements.txt in the command line where equirements.txt is located. 
## Image scraping
---
In this folder I scrape the images. The images I scrape are of celebrity. To keep it simple, I focus on one celebrity. I first wanted to scrape images of my favorite actor, which is Nicholas Cage, however Nicholas Cage is famous but not extremely famous, and therefore the amount of pictures available was not as large as I hoped for. Then I asked ChatGPT which celebrity would be better and it said Tom Cruise. But Tom Cruise images often involve aeroplanes and other vehicles, so that makes it not very suitable for image classification and generation. So then I asked ChatGPT once more and it said Julia Roberts, which turned out to work quite well. Please feel free to choose your own favorite celebrity. I also scrape a second folder of images of many celebrities that are not Julia Roberts so that it can be used for binary classification. Some of the images of Julia Roberts I scraped are not great, especially when Julia Roberts is barely visible or there are many other images available. The way you can efficiently remove faulty pictures is by using an faceboxer which turns an image of a person into an image of the face only. This is an example
![crop](scraping\roberts_images\crop/julia_roberts_01)
I generate all the corresponding faceboxes and then it is straightforward to identify the faulty picture by scrolling through the faces and looking for those that do not look like Julia Roberts. 
## Image Classification
---
In this folder, I classify images on the basis of whether or not they are Julia Roberts or not. For this I use transfer learning, in particular I use the mobilenet_v2 model pretrained by PyTorch. First, I show that the model I use for transferring actually does what it says, namely that it can tell a bear apart from an elephant (as well as classify 998 other things). Then, I use it to classify Julia Roberts or not. For this, I unfreeze the final layer of the transferred model, meaning that I trust that whatever was used in the other blocks of layers to recognize the 1000 things of this pretrained model will also work for recognizing Julia Roberts, which turns out to be the case mostly. I experiment with unfreezing 2 or 3 of the final layers, which does not make a big difference.

## Image Generation (VAE)
---
Lastly, we consider generation of images with VAE. VAE's were invented by Kingma and Welling in 2013, in a [paper](https://arxiv.org/abs/1312.6114). The idea is that you learn to sample a continuous p(x), i.e. you to obtain a smooth distribution of states x which are like the states in your input. The way you achieve this is by narrowing down your input to a very tiny latent space vector z which is learned to efficiently capture the nature of the input. However, this must be done in a randomized way, because otherwise you would have a simple autoencoder, which you could view as an overfitted version of the variational autoencoder in the sense that it is hyperfocused on a tiny amount of points in the latent space, thus making the latent space sparse. The way you do this is by letting the output of the encoder be the mean and variance of a distribution from which z is sampled, i.e. 
![sample](assets/sample.png)
In addition, you need to introduce a loss which does not only penalize if you don't reconstruct images properly but also one that makes sure that you really have a nice continuous latent space, this can be done with the loss
![sample](assets/loss.png)
Here the first term is the reconstruction error and the second is the KL-divergence which penalizes sparsity. One more crucial insight from the paper by Kingma and Welling is that if you want to do backpropagation for a model which involves the random sampling of the encoder shown in (1), you can do the following trick
![sample](assets/reparametrization.png)
which allows for separating separating the stochastic and the learnable part of the encoder output, which enables backpropagation only for the learnable part whereas the nonlearnable part is left untouched.

A VAE such as the one described above works great in theory, as well as when you try it for the MNIST database, but if you want to do serious things like image generation it turns out that you won't get far if you just construct some CNN of a couple of layers, it will likely not be able to grasp the complex nature of faces. See this example of when I tried to do this with just the loss function of (2):
![JuliaRoberts](results/vae_224_epoch25.png)
So, instead I again rely things done by others before me that worked well. In particular, there is [this amazing repo](https://github.com/LukeDitria/CNN-VAE/tree/master) where the ideas on perceptual loss expressed in [this work](https://arxiv.org/abs/1610.00291) work are put to practice. The basic idea of perceptual loss is that instead of working with the simple loss of (2), you put your decoder output through different pretrained CNN (VGG) which is trained to recognize faces, so that the loss which is extracted at several layers in this CNN can guide the VAE in a much better direction when it tries to accurately reconstruct faces (the pretrained CNN used in the loss is not trained). Here is a gif of output that I generated with the VAE trained on the CelebA dataset that shows it has learned the continuum of celebrities' faces. 
![continuum](results/morph_roster.gif).
Training one epoch of this takes 17 hours and I did 10, after which I no longer saw improvement. Now the question is how to put this to use for our ultimate goal which is sample images of Julia Roberts. The first thing I tried was to use transfer learning in a way that we saw before: I take the trained model, unfreeze some of the last layers of the decoder, and train it on my own set of Julia Roberts images. This however lead to very bad results: 
![JuliaRoberts](results/CelebA_64_epoch15.png)
The lesson learned I guess is that the CelebA set is very consrained in the way it displays celebrities' faces and even then it takes many fortnights to train, so that the model that is trained is simply too rigid to be able to adapt to being able to generate picture of a different and more diverse kind. Instead, we will simplify and hold on to what works. In particular, we will just try to look for images of Julia Roberts in the CelebA dataset and look for the value of the corresponding vector in latent space. We can then try to move around in this vector space in such a way that we leave the facial characteristics invariant to still get a continuum of different Julia Roberts'. But the CelebA dataset has 220K images (!), so how do we find images of Julia Roberts in the CelebA dataset? We use our binary classifier! I ran all the images in CelebA through the model and selected positives with a threshold for the model output of 0.9995 and still got 800 images. Out of these images, I managed to find 3 images that are Julia Roberts. 


 
